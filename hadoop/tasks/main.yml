
- name: add env in profile
  become: yes
  blockinfile:
   path: /etc/profile
   block: |
    export HADOOP_HOME=/home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export JAVA_HOME={{ java_home }}
    export ZOOKEEPER_HOME=/home/{{ zookeeper_user }}/{{ hadoop_defaultFS }}/{{ zookeeper_version }}
    export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin
      

- name: create hadoop user
  user:
   name: "{{ hadoop_user }}"
   shell: /bin/bash
   home: /home/{{ hadoop_user }}

- name: make dir for hadoop
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- import_tasks: ssh-keys-gen.yml 
- import_tasks: hosts-managment.yml

- name: make dir for hadoop data
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- name: make dir for hadoop data namenode
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data/namenode
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"  
   mode: '755'

- name: make dir for hadoop data datanode
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data/datanode
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"
   mode: '755'   

- name: make dir for hadoop data journalnode
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data/jn
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"
   mode: '755'

- name: download hadoop
  get_url:
   url: https://archive.apache.org/dist/hadoop/common/{{ hadoop_version }}/{{ hadoop_version }}.tar.gz
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- name: unarhive hadoop
  unarchive:
   src: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}.tar.gz
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/
   remote_src: yes
   owner: "{{ hadoop_user }}"

- name: generate hdfs-site.xml
  template:
   src: hdfs-site.xml.j2
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{hadoop_version}}/etc/hadoop/hdfs-site.xml

- name: generate core-site.xml
  template:
    src: core-site.xml.j2
    dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}/etc/hadoop/core-site.xml

- name: check if journalnode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep " JournalNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_journalnode_status

- name: start journalnode
  become: yes
  when: (inventory_hostname in groups['journalnode']) and (service_journalnode_status.rc == 1)
  shell: runuser -l {{ hadoop_user }} -c 'hdfs --daemon start journalnode' 

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status  

- name: format namenode
  become: yes
  when: (inventory_hostname in groups['master']) and (service_namenode_status.rc == 1) and (format_namenode == 1)
  shell: runuser -l {{ hadoop_user }} -c 'hdfs namenode -format'

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status

- name: start primary namenode
  when: (inventory_hostname in groups['master']) and (service_namenode_status.rc == 1)
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start namenode'
  become: yes

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status

- name: bootstrap secondary nodes
  when: (inventory_hostname in groups['standby']) and (service_namenode_status.rc == 1) and (bootstrap_namenode == 1)
  shell: runuser -l {{ hadoop_user }} -c 'hdfs namenode -bootstrapstandby'
  become: yes

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep " NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status  

- name: start namenodes
  when: (inventory_hostname in groups['namenode']) and (service_namenode_status.rc == 1)
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start namenode'
  become: yes
  
- name: check if datanode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep " DataNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_datanode_status

- name: start datanodes
  when:  (inventory_hostname in groups['datanodes']) and (service_namenode_status.rc == 1)
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start datanode'
  become: yes

- name: check if zkfc is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "ZKFailoverController" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_zkfc_status
  
- name: start zkfc format on NN1
  when: inventory_hostname in groups['master'] and (service_zkfc_status.rc == 1) and (format_zkfc == 1)
  shell: runuser -l  {{ hadoop_user }} -c ' hdfs zkfc -formatZK'
  become: yes

- name: check if zkfc is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "ZKFailoverController" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_zkfc_status
  
- name: start zkfc on namenodes 
  when: (service_zkfc_status.rc == 1) and (inventory_hostname in groups['namenode'])
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start zkfc'
  become: yes
