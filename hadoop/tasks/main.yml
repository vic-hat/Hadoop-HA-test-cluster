
- name: add env in profile
  become: yes
  blockinfile:
   path: /etc/profile
   block: |
    export HADOOP_HOME=/home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export JAVA_HOME={{ java_home }}
    export ZOOKEEPER_HOME=/home/{{ zookeeper_user }}/{{ hadoop_defaultFS }}/{{ zookeeper_version }}
    export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin 
    export  _JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true

- name: create hadoop user
  user:
   name: "{{ hadoop_user }}"
   shell: /bin/bash
   home: /home/{{ hadoop_user }}
   generate_ssh_key: yes
   ssh_key_bits: 2048
   ssh_key_file: .ssh/id_rsa

- name: make dir for hadoop
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- import_tasks: ssh-keys-gen2.yml 
- import_tasks: hosts-managment.yml

- name: make dir for hadoop data
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- name: make dir for hadoop namenode, datanode and journalnode data
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data/{{ item }}
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"  
   mode: '755'
  with_items: 
   - datanode
   - namenode
   - jn   

- name: download hadoop
  get_url:
   url: https://archive.apache.org/dist/hadoop/common/{{ hadoop_version }}/{{ hadoop_version }}.tar.gz
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- name: unarhive hadoop
  unarchive:
   src: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}.tar.gz
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/
   remote_src: yes
   owner: "{{ hadoop_user }}"
   #exclude: 
   #- hdfs-site.xml
   #- core-site.xml

- name: generate hdfs-site.xml
  template:
   src: hdfs-site.xml.j2
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{hadoop_version}}/etc/hadoop/hdfs-site.xml

- name: generate core-site.xml
  template:
    src: core-site.xml.j2
    dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}/etc/hadoop/core-site.xml

- name: generate hadoop service file for systemd
  template:
   src: hadoop@.service.j2
   dest: /etc/systemd/system/hadoop@.service
  become: yes

- name: enable hadoop@zkfc.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['namenode']
  shell: systemctl enable hadoop@zkfc.service

- name: enable hadoop@namenode.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['namenode']
  shell: systemctl enable hadoop@namenode.service

- name: enable hadoop@datanode.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['datanode']
  shell: systemctl enable hadoop@datanode.service

- name: enable hadoop@journalnode.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['namenode']
  shell: systemctl enable hadoop@journalnode.service
  
- name: check if journalnode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep " JournalNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_journalnode_status

- name: start journalnode
  become: yes
  when: (inventory_hostname in groups['journalnode']) and (service_journalnode_status.rc == 1)
  shell: runuser -l {{ hadoop_user }} -c 'hdfs --daemon start journalnode' 

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status  

- name: format namenode
  become: yes
  when: (inventory_hostname in groups['master']) and (service_namenode_status.rc == 1) and (format_namenode == 1)
  shell: runuser -l {{ hadoop_user }} -c 'hdfs namenode -format'

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status

- name: start primary namenode
  when: (inventory_hostname in groups['master']) and (service_namenode_status.rc == 1)
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start namenode'
  become: yes

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status

- name: bootstrap secondary nodes
  when: (inventory_hostname in groups['standby']) and (service_namenode_status.rc == 1) and (bootstrap_namenode == 1)
  shell: runuser -l {{ hadoop_user }} -c 'hdfs namenode -bootstrapstandby'
  become: yes

- name: check if namenode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "NameNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_namenode_status  

- name: start namenodes
  when: (inventory_hostname in groups['namenode']) and (service_namenode_status.rc == 1)
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start namenode'
  become: yes
  
- name: check if datanode is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "DataNode" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_datanode_status

- name: start datanodes
  when:  (inventory_hostname in groups['datanode']) and (service_namenode_status.rc == 1)
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start datanode'
  become: yes

- name: check if zkfc is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "ZKFailoverController" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_zkfc_status
  
- name: start zkfc format on NN1
  when: inventory_hostname in groups['master'] and (service_zkfc_status.rc == 1) and (format_zkfc == 1)
  shell: runuser -l  {{ hadoop_user }} -c ' hdfs zkfc -formatZK'
  become: yes

- name: check if zkfc is running
  become: yes
  shell: runuser -l {{ hadoop_user }} -c 'jps | grep "ZKFailoverController" | grep -v grep'
  ignore_errors: yes
  changed_when: false
  register: service_zkfc_status
  
- name: start zkfc on namenodes 
  when: (service_zkfc_status.rc == 1) and (inventory_hostname in groups['namenode'])
  shell: runuser -l  {{ hadoop_user }} -c 'hdfs --daemon start zkfc'
  become: yes


  

  

  
