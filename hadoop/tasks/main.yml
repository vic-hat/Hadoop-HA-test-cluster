
- name: add env in profile
  become: yes
  blockinfile:
   path: /etc/profile
   block: |
    export HADOOP_HOME=/home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export JAVA_HOME={{ java_home }}
    export ZOOKEEPER_HOME=/home/{{ zookeeper_user }}/{{ hadoop_defaultFS }}/{{ zookeeper_version }}
    export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZOOKEEPER_HOME/bin 
    export  _JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true

- name: create hadoop user
  user:
   name: "{{ hadoop_user }}"
   shell: /bin/bash
   home: /home/{{ hadoop_user }}
   generate_ssh_key: yes
   ssh_key_bits: 2048
   ssh_key_file: .ssh/id_rsa

- name: make dir for hadoop
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- import_tasks: ssh-keys-gen2.yml 
- import_tasks: hosts-managment.yml

- name: make dir for hadoop data
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- name: make dir for hadoop namenode, datanode and journalnode data
  file:
   path: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/data/{{ item }}
   state: directory
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"  
   mode: '755'
  with_items: 
   - datanode
   - namenode
   - jn   

- name: download hadoop
  get_url:
   url: https://archive.apache.org/dist/hadoop/common/{{ hadoop_version }}/{{ hadoop_version }}.tar.gz
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}
   owner: "{{ hadoop_user }}"
   group: "{{ hadoop_group }}"

- name: unarhive hadoop
  unarchive:
   src: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}.tar.gz
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/
   remote_src: yes
   owner: "{{ hadoop_user }}"
   #exclude: 
   #- hdfs-site.xml
   #- core-site.xml

- name: generate hdfs-site.xml
  template:
   src: hdfs-site.xml.j2
   dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{hadoop_version}}/etc/hadoop/hdfs-site.xml

- name: generate core-site.xml
  template:
    src: core-site.xml.j2
    dest: /home/{{ hadoop_user }}/{{ hadoop_defaultFS }}/{{ hadoop_version }}/etc/hadoop/core-site.xml

- name: generate hadoop service file for systemd
  template:
   src: hadoop@.service.j2
   dest: /etc/systemd/system/hadoop@.service
  become: yes

- name: enable hadoop@zkfc.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['namenode']
  shell: systemctl enable hadoop@zkfc.service

- name: enable hadoop@namenode.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['namenode']
  shell: systemctl enable hadoop@namenode.service

- name: enable hadoop@datanode.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['datanode']
  shell: systemctl enable hadoop@datanode.service

- name: enable hadoop@journalnode.service
  become: yes
  become_method: sudo
  when: inventory_hostname in groups['namenode']
  shell: systemctl enable hadoop@journalnode.service
  

  

  
